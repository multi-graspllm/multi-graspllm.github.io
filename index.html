<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description" content="Multi-hand LLM-Based Grasp Generation with Semantic Information Guided">
  <meta name="keywords" content="Robotics,Grasp Generation,Multi-hand,Dextrous Hand">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Multi-GraspLLM: A Multimodal LLM for Multi-Hand Semantic Guided Grasp Generation</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag() {
      dataLayer.push(arguments);
    }
    gtag('js', new Date());
    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">
  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.svg">
  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
  <style>

    .abstract-image {
      background-color: transparent !important;
    width: 100%;  /* 图片宽度自适应列宽 */
    height: auto; /* 保持宽高比 */
    display: block;
    margin: 0 auto; /* 水平居中 */
    }
    .content {
    text-align: left; /* 强制内容靠左对齐 */
  }


  .section {
  background-color: transparent !important; /* 删除灰色背景 */
  border: none !important; /* 删除边框 */
  box-shadow: none !important; /* 删除阴影 */
}

/* 移除图片样式 */
.interpolation-image {
  background-color: transparent !important; /* 确保图片没有背景色 */
  border: none !important; /* 确保图片没有边框 */
  display: block; /* 保持图片正常显示 */
  margin: 0 auto; /* 居中显示 */
}

/* 修正 p 标签默认样式 */
p {
  background-color: transparent !important; /* 确保文字背景透明 */
  border: none !important;
  box-shadow: none !important;
}

.long-cross {
    color: black; /* 星号颜色 */
    font-size: 1.5em; /* 星号大小 */
    margin-left: 5px; /* 星号与文字的间距 */
  }



  </style>
  
</head>
<body>

<nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
  <div class="navbar-menu">
    <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
      <a class="navbar-item" href="https://scholar.google.com/citations?hl=zh-CN&user=GtKG4asAAAAJ">
      <span class="icon">
          <i class="fas fa-home"></i>
      </span>
      </a>

      <div class="navbar-item has-dropdown is-hoverable">
        <a class="navbar-link">
          More Research
        </a>
        <div class="navbar-dropdown">
          <a class="navbar-item" href="https://arxiv.org/abs/2410.08901">
            SegGrasp
          </a>
        </div>
      </div>
    </div>
  </div>
</nav>

<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">Multi-GraspLLM: A Multimodal LLM for Multi-Hand Semantic Guided Grasp Generation</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block"><a href="https://scholar.google.com/citations?hl=zh-CN&user=GtKG4asAAAAJ">Haosheng Li</a><sup>1*</sup>,</span>
            <span class="author-block"><a href="https://scholar.google.com/citations?user=6-bYm5EAAAAJ&hl=zh-CN&oi=ao">Weixin Mao</a><sup>2*</sup>,</span>
            <span class="author-block"><a href="https://scholar.google.com/citations?user=JBMI5yQAAAAJ&hl=zh-CN&oi=ao">Weipeng Deng</a><sup>3</sup>,</span>
            <span class="author-block"><a href="">Chenyu Meng</a><sup>1</sup>,</span>
            <span class="author-block"><a href="https://scholar.google.com/citations?user=bzzBut4AAAAJ&hl=zh-CN&oi=ao">Haoqiang Fan</a><sup>4</sup>,</span>
            <span class="author-block"><a href="https://scholar.google.com/citations?user=YI0sRroAAAAJ&hl=zh-CN&oi=ao">Tiancai Wang</a><sup>4</sup>,</span>
            <span class="author-block"><a href="https://ece.hkust.edu.hk/pingtan">Ping Tan</a><sup>5</sup>,</span>
            <span class="author-block"><a href="">Hongan Wang</a><sup>1</sup>,</span>
            <span class="author-block"><a href="https://www.idengxm.com/">Xiaoming Deng</a><sup>1✝</span></sup></span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>Institute of Software, Chinese Academy of Sciences</span>
            <span class="author-block"><sup>2</sup>Waseda University</span>
            <span class="author-block"><sup>3</sup>Hongkong University</span>
            <span class="author-block"><sup>4</sup>MEGVII Technology</span>
            <span class="author-block"><sup>5</sup>Hong Kong University of Science and Technology </span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <span class="link-block">
                <a href="" class="external-link button is-normal is-rounded is-dark">
                  <span class="icon"><i class="ai ai-arxiv"></i></span>
                  <span>arXiv</span>
                </a>
              </span>
              
              <span class="link-block">
                <a href="https://huggingface.co/datasets/LHS-LHS/Multi-GraspSet" class="external-link button is-normal is-rounded is-dark">
                  <span class="icon"><i class="far fa-images"></i></span>
                  <span>DataSet(Undergoing Updates)</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://github.com/multi-graspllm/Multi-GraspLLM" class="external-link button is-normal is-rounded is-dark">
                  <span class="icon"><i class="fab fa-github"></i></span>
                  <span>Code(Undergoing Updates)</span>
                </a>
              </span>


            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<div class="content has-text-justified"></div>
<!-- 
<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-four-fifths">
      
        <div class="columns is-centered">
          <div class="column is-half has-text-centered">
            <img src="./static/picture/overview_output_01.png" class="abstract-image" alt="Abstract Image"/>
          </div>
        </div>
      
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Multi-hand semantic grasp generation aims to generate feasible and semantically appropriate grasp poses for different robotic hands based on natural language instructions. 
            Although the task is highly valuable, due to the lack of multi-hand grasp datasets with fine-grained contact description between robotic hands and objects, it is still a long-standing difficult task. 
            In this paper, we present Multi-GraspSet, the first large-scale multi-hand grasp dataset with automatically contact annotations. 
            Based on Multi-GraspSet, we propose Multi-GraspLLM, a unified language-guided grasp generation framework. It leverages large language models (LLM) to handle variable-length sequences, generating grasp poses for diverse robotic hands in a single unified architecture. 
            Multi-GraspLLM first aligns the encoded point cloud features and text features into a unified semantic space.
             It then generates grasp bin tokens which are subsequently converted into grasp pose for each robotic hand via hand-aware linear mapping. 
            The experimental results demonstrate that our approach significantly outperforms existing methods on Multi-GraspSet.  
          </p>
        </div>
      </div>
    </div>
  </div>
</section> -->
<section class="section">
  <div class="container is-max-desktop">
    <!-- 图片 -->
    <div class="columns is-centered">
      <div class="column is-three-fifths has-text-centered">
        <img src="./static/picture/overview_output_01.png" class="abstract-image" alt="Abstract Image"/>
      </div>
    </div>

    <!-- Abstract 内容 -->
    <div class="columns">
      <div class="column is-full-width">
        <h2 class="title is-3">Abstract</h2>
        <div class="content">
          <p>
            Multi-hand semantic grasp generation aims to generate feasible and semantically appropriate grasp poses for different robotic hands based on natural language instructions. 
            Although the task is highly valuable, due to the lack of multi-hand grasp datasets with fine-grained contact description between robotic hands and objects, it is still a long-standing difficult task. 
            In this paper, we present Multi-GraspSet, the first large-scale multi-hand grasp dataset with automatically contact annotations. 
            Based on Multi-GraspSet, we propose Multi-GraspLLM, a unified language-guided grasp generation framework. It leverages large language models (LLM) to handle variable-length sequences, generating grasp poses for diverse robotic hands in a single unified architecture. 
            Multi-GraspLLM first aligns the encoded point cloud features and text features into a unified semantic space.
             It then generates grasp bin tokens which are subsequently converted into grasp pose for each robotic hand via hand-aware linear mapping. 
            The experimental results demonstrate that our approach significantly outperforms existing methods on Multi-GraspSet.  
          </p>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- <section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Dataset Construction</h2>
        <div class="content has-text-justified">
          <p>
            The initial unified grasp generation produces physically stable grasps. Then, through two levels of annotations, we generate pair data containing basic conversation with corresponding grasp pose for each robotics hand.
          </p>
        </div>
        <div class="columns is-vcentered interpolation-panel">
          <div class="column is-full-width has-text-centered">
            <img src="./static/picture/dataset_construction.png" class="interpolation-image" alt="Dataset Construction Image"/>
            <p>Dataset Construction</p>
          </div>
        </div>
      </div>
    </div>
  </div>
</section> -->

<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Dataset Construction</h2>
        <div class="content has-text-justified">
          <p>
            The initial unified grasp generation produces physically stable grasps. Then, through two levels of annotations, we generate pair data containing basic conversation with corresponding grasp pose for each robotics hand.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- 第二部分：单张全宽图片 -->
<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full-width">
        <div class="columns is-centered is-vcentered">
          <div class="column is-full-width has-text-centered">
            <img src="./static/picture/dataset_construction.png" class="interpolation-image" alt="Dataset Construction Image"/>
            <p>Dataset Construction</p>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>



<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Multi-GraspLLM</h2>
        <div class="content has-text-justified">
          <p>
            The point encoder extracts point clouds from objects and maps them with language descriptions into the same latent space. The LLM backbone then generate grasp bin tokens as output. Finally, we convert these grasp bin tokens into corresponding grasp angles for each robotic hand.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>



<!-- 第二部分：单张全宽图片 -->
<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full-width">
        <div class="columns is-centered is-vcentered">
          <div class="column is-full-width has-text-centered">
            <img src="./static/picture/pipline-squeeze_01.png" class="interpolation-image" alt="Pipeline Image"/>
            <p>Pipeline of Multi-GraspLLM</p>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>



<!-- <section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Visualization</h2>
        <div class="content has-text-justified">
 
        </div>
      </div>
    </div>
  </div>
</section> -->

<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Visualization</h2>
        <h3 class="title is-4"> Multi-GraspSet and Training data Templates</h3>
        <div class="content has-text-justified">
          <p>
            Multi-GraspSet includes grasp poses of common objects across three robotic hands with
            two types of contact annotations. Based on this dataset, we generate training data for Multi-GraspLLM, consisting of three different categories.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full-width">
        <div class="columns is-centered is-vcentered">
          
          <div class="column is-half has-text-centered">
            <img src="./static/picture/vis_dataset_grasp_output_01.png" class="interpolation-image" alt="Grasp Visualization Image 2"/>
            <p>Illustration of Multi-GraspSet</p>
          </div>

          <div class="column is-half has-text-centered">
            <img src="./static/picture/train_data_01.png" class="interpolation-image" alt="Grasp Visualization Image 1"/>
            <p>Training Data Templates</p>
          </div>
        </div>
      </div>
    </div>
  </div>
 </section>



 <section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h3 class="title is-4"> Multi-GraspLLM Results</h3>
        <div class="content has-text-justified">
          <p>
            Multi-GraspLLM can generate different robotic hand grasping poses based on instructions with varying levels of contact information.For
            the low-level instruction, model generates grasps without any contact information, while mid-level focuses on the object part information. The high-level instruction utilizes finger contact information, enabling our model to simply control individual fingers.</p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- 第二部分：单张全宽图片 -->
<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full-width">
        <div class="columns is-centered is-vcentered">
          <div class="column is-full-width has-text-centered">
            <img src="./static/picture/Supp_visDataset.png" class="interpolation-image" alt="Visualization Image"/>
            <p>Visualization of the Multi-GraspLLM Results</p>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Real World Experiment</h2>
        <div class="content has-text-justified">
          <p>
            We conducted extensive experiments in real world environments. we collect 11 objects from daily life covering from tools to potted plant.
            We tested both the baseline method for grippers and Multi-GraspLLM on these objects, evaluating them based on grasping success rate and part selection accuracy.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full-width">
        <div class="columns is-centered is-vcentered">
          <div class="column is-full-width has-text-centered">
            <img src="./static/picture/real.png" class="interpolation-image" alt="Real World Experiment For Grippper"/>
            <p>Real World Experiment For Grippper</p>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <!-- <pre><code>@article{park2021nerfies,
  author    = {Park, Keunhong and Sinha, Utkarsh and Barron, Jonathan T. and Bouaziz, Sofien and Goldman, Dan B and Seitz, Steven M. and Martin-Brualla, Ricardo},
  title     = {Nerfies: Deformable Neural Radiance Fields},
  journal   = {ICCV},
  year      = {2021},
}</code></pre> -->
  </div>
</section>

<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <!-- <a class="icon-link" href="https://github.com/keunhong" class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a> -->
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/">Creative Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
          <p>
            This means you are free to borrow the <a href="https://github.com/nerfies/nerfies.github.io">source code</a> of this website,
            we just ask that you link back to this page in the footer.
            Please remember to remove the analytics code included in the header of the website which
            you do not want on your website.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
